
--- File Path: /Users/samuel.shapley/OrchestrAI/ai.py ---
import waveimport mathimport numpy as npimport tempfileimport jsonimport openaiopenai.api_key = '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'import osclass AI: def __init__(self, system="", model = 'gpt-4', openai=openai): self.system = system self.model = model self.openai = openai self.messages = [{"role": "system", "content": system}] def generate_response(self, prompt): self.messages.append({"role": "user", "content": prompt}) response_json = self.openai.ChatCompletion.create( model=self.model, messages=self.messages, ) response_text = response_json["choices"][0]["message"]["content"] self.messages.append({"role": "assistant", "content": response_text}) return response_text, self.messages def generate_image(self, prompt, n=1, size="1024x1024", response_format="url"): """Generate an image using DALLÂ·E given a prompt. Arguments: prompt (str): A text description of the desired image(s). n (int, optional): The number of images to generate. Defaults to 1. size (str, optional): The size of the generated images. Defaults to "1024x1024". response_format (str, optional): The format in which the generated images are returned. Defaults to "url". Returns: dict: The response from the OpenAI API. """ return openai.Image.create(prompt=prompt, n=n, size=size, response_format=response_format)

--- File Path: /Users/samuel.shapley/OrchestrAI/orchestrate.py ---
# orchestrate.pyimport yamlimport networkx as nximport modules# Load pipeline.yml from pipelines folderwith open('pipelines/pipeline.yml') as f: pipeline = yaml.safe_load(f)# Create the DAGG = nx.DiGraph()# Data dictionary to store the outputs of each moduledata_dict = {}for operation in pipeline['pipeline']: module_name = operation['module'] output_name = operation['output_name'] inputs = operation['inputs'] # Add node for this module if it doesn't already exist G.add_node(module_name) # Add edges for inputs for i in inputs: G.add_edge(i, module_name) G.add_edge(module_name, output_name)# Now you can use topological sort to get the execution order:execution_order = list(nx.topological_sort(G))# And execute the tasks in this order, passing the necessary data between them:for operation in pipeline['pipeline']: module_name = operation['module'] # Print the module name in red print(f"\033[91m{module_name.upper()}\033[00m") if hasattr(modules, module_name): module_func = getattr(modules, module_name) prompt = '\n'.join([data_dict.get(input, '') for input in operation['inputs']]) result, messages = module_func(prompt) data_dict[operation['output_name']] = result print(result) else: print(f"Warning: No module function '{module_name}'. Ignoring.")

--- File Path: /Users/samuel.shapley/OrchestrAI/modules.py ---
# modules.pyfrom ai import AIimport osdef load_system_prompt(module_name): with open(f'system_prompts/{module_name}.txt', 'r') as file: return file.read().replace('\n', '') def start_module(prompt): print("Please specify the task you want to perform:") return input(prompt), []def human_intervention(prompt): print("Please provide additional information to guide the agent:") return input(prompt), []def task_planner(prompt): system_prompt = load_system_prompt("task_planner") ai = AI(system=system_prompt, model='gpt-4') return ai.generate_response(prompt)def scrutinizer(prompt): system_prompt = load_system_prompt("scrutinizer") ai = AI(system=system_prompt, model='gpt-4') return ai.generate_response(prompt)def enhancer(prompt): system_prompt = load_system_prompt("enhancer") ai = AI(system=system_prompt, model='gpt-4') return ai.generate_response(prompt)# Define other module functions here

If you understand, generate only YES.