
--- File Path: /Users/samuel.shapley/OrchestrAI/memory.py ---
import jsonimport datetimeclass Logger: def __init__(self): # Initialize an empty list for storing logs self.logs = [] def log_action(self, module, prompt, response, model): timestamp = datetime.datetime.now().isoformat() action_log = { "module": module, "prompt": prompt, "response": response, "model": model, "timestamp": timestamp, } # Append the action log to the logs list self.logs.append(action_log) # Update log file self.update_log_file() def update_log_file(self): # Save logs to a JSON file with open('memory_log.json', 'w') as f: json.dump(self.logs, f)

--- File Path: /Users/samuel.shapley/OrchestrAI/ai.py ---
import jsonimport openaiopenai.api_key = ''import osimport datetimeimport jsonfrom logging import Loggerclass AI: def __init__(self, system="", model = 'gpt-4', openai=openai): self.system = system self.model = model self.openai = openai self.messages = [{"role": "system", "content": system}] def generate_response(self, prompt): self.messages.append({"role": "user", "content": prompt}) response = self.openai.ChatCompletion.create( model=self.model, stream=True, messages=self.messages, ) chat = [] for chunk in response: delta = chunk["choices"][0]["delta"] # type: ignore msg = delta.get("content", "") print(msg, end="") chat.append(msg) print() response_text = "".join(chat) self.messages.append({"role": "assistant", "content": response_text}) return response_text, self.messages def generate_image(self, prompt, n=1, size="1024x1024", response_format="url"): """Generate an image using DALLÂ·E given a prompt. Arguments: prompt (str): A text description of the desired image(s). n (int, optional): The number of images to generate. Defaults to 1. size (str, optional): The size of the generated images. Defaults to "1024x1024". response_format (str, optional): The format in which the generated images are returned. Defaults to "url". Returns: dict: The response from the OpenAI API. """ return openai.Image.create(prompt=prompt, n=n, size=size, response_format=response_format)

--- File Path: /Users/samuel.shapley/OrchestrAI/pipeline.yml ---
pipeline: - module: start_module inputs: [] output_name: request - module: task_planner inputs: [request] output_name: task_plan - module: scrutinizer inputs: [request, task_plan] output_name: scrutinized_task_plan - module: enhancer inputs: [request, scrutinized_task_plan, task_plan] output_name: enhanced_task_plan - module: code_planner inputs: [request, enhanced_task_plan] supplement: "Use only python." output_name: code_plan - module: engineer inputs: [code_plan] output_name: code - module: debugger inputs: [code] output_name: debugged_code

--- File Path: /Users/samuel.shapley/OrchestrAI/orchestrate.py ---
# orchestrate.pyimport yamlimport networkx as nximport modules# Load pipeline.yml from pipelines folderwith open('pipeline.yml') as f: pipeline = yaml.safe_load(f)# Create the DAGG = nx.DiGraph()# Data dictionary to store the outputs of each moduledata_dict = {}for operation in pipeline['pipeline']: module_name = operation['module'] output_name = operation['output_name'] inputs = operation['inputs'] supplement = operation.get('supplement', '') # Add node for this operation's output if it doesn't already exist G.add_node(output_name) # Add edges for inputs for i in inputs: G.add_edge(i, output_name)# Now you can use topological sort to get the execution order:execution_order = list(nx.topological_sort(G))# And execute the tasks in this order, passing the necessary data between them:for output_name in execution_order: if output_name in data_dict: # skip over inputs that are already defined continue operation = [item for item in pipeline['pipeline'] if item['output_name'] == output_name][0] module_name = operation['module'] supplement = operation.get('supplement', '') # Print the module name in red print(f"\033[91m{module_name.upper()}\033[00m") if hasattr(modules, module_name): module_func = getattr(modules, module_name) prompt = '\n'.join([data_dict.get(input, '') for input in operation['inputs']]) prompt += '\n Additional User Input' + supplement result, messages = module_func(prompt) data_dict[output_name] = result else: print(f"Warning: No module function '{module_name}'. Ignoring.")

--- File Path: /Users/samuel.shapley/OrchestrAI/README.md ---
# OrchestrAIOrchestrAI is a system built in Python that enables complex task execution by orchestrating interactions between multiple instances of OpenAI's GPT-4 model. It uses the `networkx` library to handle dependencies between various AI modules, and YAML to define and manage the task pipelines.## Getting Started### PrerequisitesTo run OrchestrAI, you'll need:- Python 3.7 or later- OpenAI Python library- networkx- PyYAMLYou can install these with pip:```bashpip install -r requirements.txt```### ConfigurationTo set up OrchestrAI, you need to organize the following files:- `ai.py` - This file contains the AI class, which manages interactions with the OpenAI GPT-4 model. Your OpenAI API key must be set in this file.- `modules.py` - This script lists the available AI modules.- `orchestrate.py` - This main script loads modules and pipelines, constructs a Directed Acyclic Graph (DAG) of operations, and executes them in the correct order.- `helpers.py` - This script provides helper functions, including loading system prompts, parsing chat data, and writing code files.- `pipeline.yml` - This YAML file describes the sequence of operations to be executed in your pipeline.### Setting Up a Pipeline1. Define your pipeline in the `pipeline.yml` file. Each operation in the pipeline consists of a `module` and an `output_name`. The `module` represents a specific task performed by an AI, and the `output_name` is the output of that task, which can be used as input for subsequent operations. Here's an example of a simple pipeline:```yamlpipeline:- module: start_module inputs: [] output_name: request- module: code_planner inputs: [request] output_name: code_plan- module: engineer inputs: [code_plan] output_name: code```2. Save and close the `pipeline.yml` file.### Running the ScriptTo run OrchestrAI, execute `orchestrate.py`:```bashpython orchestrate.py```The script will execute the operations in the pipeline in the order specified, querying the GPT-4 model as necessary and storing the results.## Understanding the ModulesThe `modules.py` file contains different AI modules, each responsible for a specific type of operation, such as `start_module`, `human_intervention`, `task_planner`, `scrutinizer`, `enhancer`, `code_planner`, `debugger`, and `engineer`.Each module interacts with the GPT-4 model to execute its specific task, and the output is stored for use in subsequent operations as defined in the pipeline.## ContributingContributions to OrchestrAI are welcome! Please submit a pull request with your changes, and be sure to include tests and documentation.## LicenseOrchestrAI is open-source software, released under the MIT License.

--- File Path: /Users/samuel.shapley/OrchestrAI/modules.py ---
# modules.pyimport helpers as hfrom ai import AIimport osfrom memory import Loggerlogger = Logger() def start_module(prompt): print("\033[92mPlease specify the task you want to perform:\033[00m") start = input() logger.log_action("start", start, None, None) return prompt + start, []def human_intervention(prompt): module_name = "human_intervention" print("Please provide additional information to guide the agent:") additional_info = input() logger.log_action(module_name, prompt, None, None) return prompt + additional_info, []def task_planner(prompt): module_name = "task_planner" system_prompt = h.load_system_prompt(module_name) ai = AI(system=system_prompt, model='gpt-4') response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') return response, messagesdef scrutinizer(prompt): module_name = "scrutinizer" system_prompt = h.load_system_prompt(module_name) ai = AI(system=system_prompt, model='gpt-4') response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') return response, messagesdef enhancer(prompt): module_name = "enhancer" system_prompt = h.load_system_prompt(module_name) ai = AI(system=system_prompt, model='gpt-4') response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') return response, messagesdef code_planner(prompt): module_name = "code_planner" system_prompt = h.load_system_prompt(module_name) ai = AI(system=system_prompt, model='gpt-4') print("\033[93mPlanning code...\033[00m") response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') return response, messagesdef debugger(prompt): module_name = "debugger" system_prompt = h.load_system_prompt("debugger") ai = AI(system=system_prompt, model='gpt-4') print("\033[93mDebugging code...\033[00m") response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') return response, messagesdef engineer(prompt): module_name = "engineer" system_prompt = h.load_system_prompt(module_name) ai = AI(system=system_prompt, model='gpt-4') print("\033[93mGenerating code...\033[00m") response, messages = ai.generate_response(prompt) logger.log_action(module_name, prompt, response, 'gpt-4') # Parse the chat and extract files print("\033[95mExtracting code...\033[00m") files = h.parse_chat(response) # Save files to disk h.to_files(files) # Generate repo promtpt. return response, messagesdef debugger(prompt): # Run the requirements.txt file in generated_code folder with pip3 print("\033[94mInstalling dependencies...\033[00m") os.system("pip3 install -r generated_code/requirements.txt") print("\033[92mDependencies Done!\033[00m") # In a while loop, run main.py, and if it crashes, pass the full codebase to the debugger. print("\033[91mRunning code...\033[00m") os.system("python3 generated_code/main.py")

--- File Path: /Users/samuel.shapley/OrchestrAI/helpers.py ---
import osimport redef load_system_prompt(module_name): # Load the generic system prompt with open('general_system.txt', 'r') as file: system_prompt = file.read().replace('\n', '') with open(f'system_prompts/{module_name}.txt', 'r') as file: module_prompt = file.read().replace('\n', '') system_prompt = system_prompt + '\n' + module_name.upper() + '\n' + module_prompt + '\n' return system_promptdef parse_chat(chat): # Get all ``` blocks and preceding filenames regex = r"(\S+)\n\s*```[^\n]*\n(.+?)```" matches = re.finditer(regex, chat, re.DOTALL) files = [] for match in matches: # Follow similar steps as your original function... path = re.sub(r"[<>'|?*]", "", match.group(1)) path = re.sub(r"^\[(.*)\]$", r"\1", path) path = re.sub(r"^`(.*)`$", r"\1", path) path = re.sub(r"\]$", "", path) code = match.group(2) files.append((path, code)) return filesdef to_files(files): if not os.path.exists("generated_code"): os.mkdir("generated_code") for file_name, file_content in files: with open(os.path.join("generated_code", file_name), "w") as file: file.write(file_content)

If you understand, generate only YES.